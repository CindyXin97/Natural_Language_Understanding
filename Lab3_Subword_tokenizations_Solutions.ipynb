{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab3 Subword-tokenizations.Solutions",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKyn4Uuk2_gt",
        "colab_type": "text"
      },
      "source": [
        "# Subword Tokenization\n",
        "\n",
        "We will use a subword tokenization library from Huggingface.\n",
        "Let's install the tokenizer and download the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_xCE5TC0ljj",
        "colab_type": "code",
        "outputId": "33116240-b58e-44c1-9708-86c9c02e17c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "!pip install tokenizers\n",
        "# download some sample text \n",
        "!wget https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\n",
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "--2020-02-13 03:56:40--  https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 278779 (272K) [text/plain]\n",
            "Saving to: ‘botchan.txt.2’\n",
            "\n",
            "botchan.txt.2       100%[===================>] 272.25K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2020-02-13 03:56:41 (4.84 MB/s) - ‘botchan.txt.2’ saved [278779/278779]\n",
            "\n",
            "--2020-02-13 03:56:41--  https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.12.6\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.12.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 213450 (208K) [text/plain]\n",
            "Saving to: ‘bert-base-cased-vocab.txt.2’\n",
            "\n",
            "bert-base-cased-voc 100%[===================>] 208.45K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2020-02-13 03:56:41 (5.40 MB/s) - ‘bert-base-cased-vocab.txt.2’ saved [213450/213450]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbEWzm3cipRr",
        "colab_type": "text"
      },
      "source": [
        "# Import Tokenizer Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxO4woDBPZUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tokenizers import (ByteLevelBPETokenizer,\n",
        "                            SentencePieceBPETokenizer,\n",
        "                            BertWordPieceTokenizer)\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4bFr_m7dRad",
        "colab_type": "text"
      },
      "source": [
        "# First let's try out a pretrained WordPiece tokenizer used in BERT model.\n",
        "\n",
        "You will notice the [CLS] and [SEP] tokens which are usually added at the beginning and end of a sentence in BERT model. You will learn more about BERT in lecture.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4fV-65_dYXp",
        "colab_type": "code",
        "outputId": "1b8f4d5e-56a1-48db-8243-bf0508ea625e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Intialize tokenizer with BERT's BPE vocabulary\n",
        "bert_tokenizer = BertWordPieceTokenizer(\"bert-base-cased-vocab.txt\")\n",
        "\n",
        "# Tokenize text\n",
        "output = bert_tokenizer.encode(\"Hellooooooooo! How are you?\")\n",
        "print(output.tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', 'hello', '##oo', '##oo', '##oo', '##oo', '!', 'how', 'are', 'you', '?', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBQUJ4jFerk6",
        "colab_type": "text"
      },
      "source": [
        "# Now, let's train your own custom WordPiece tokenizer.\n",
        "We downloaded a file named `botchan.txt` at the beginning. Let's train a wordpiece model on this file.  \n",
        "\n",
        "Recap: WordPiece expects the pretokenized word sequence.  \n",
        "The `tokenizer` library has built-in pretokenizer for WordPiece model so you don't have to worry about it. But, if you want to, you can specify the type of builtin pretokenizer you want to use. \n",
        "\n",
        "You can check the list of built-in pretokenizers [here](https://github.com/huggingface/tokenizers/blob/master/bindings/python/src/pre_tokenizers.rs)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cKKrg9ffZlw",
        "colab_type": "code",
        "outputId": "50f65419-060f-42eb-98e0-da270ee290eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# First, initialize a new WordPiece tokenizer\n",
        "my_tokenizer = BertWordPieceTokenizer()\n",
        "\n",
        "# Customize pre-tokenization and decoding\n",
        "my_tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit(add_prefix_space=True)\n",
        "my_tokenizer.decoder = decoders.WordPiece()\n",
        "\n",
        "# And then train on some text file\n",
        "my_tokenizer.train([\"botchan.txt\"])\n",
        "\n",
        "# Now we can encode\n",
        "my_tok_output = my_tokenizer.encode(\"Hellooooooooo! How are you?\")\n",
        "print(\"tokens: \", my_tok_output.tokens)\n",
        "print(\"offsets:\", my_tok_output.offsets)\n",
        "print(\"getting a token from offsets:\", my_tok_output.original_str[my_tok_output.offsets[0]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokens:  ['hello', '##oo', '##oo', '##oo', '##oo', '!', 'how', 'are', 'you', '?']\n",
            "offsets: [(0, 5), (5, 7), (7, 9), (9, 11), (11, 13), (13, 14), (15, 18), (19, 22), (23, 26), (26, 27)]\n",
            "getting a token from offsets: Hello\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvMNp2QifoqP",
        "colab_type": "text"
      },
      "source": [
        "You can see that the output is different from pretrained Bert's WordPiece tokenizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekDr7a0duz-s",
        "colab_type": "text"
      },
      "source": [
        "# Now, let's train a SentencePiece Tokenizer.\n",
        "\n",
        "SentencePieceBPETokenizer is already implemented for you so you don't have to define trainer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXZsSWBwWulR",
        "colab_type": "code",
        "outputId": "109b4e36-4b83-4867-dee5-85f2d7bd47a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# First, initialize a new SentencePiece tokenizer\n",
        "my_sp_tokenizer = SentencePieceBPETokenizer()\n",
        "\n",
        "# And then train\n",
        "my_sp_tokenizer.train([\"botchan.txt\"], vocab_size=20000, min_frequency=2)\n",
        "\n",
        "# Now we can encode\n",
        "my_sp_tok_output = my_sp_tokenizer.encode(\"Hellooooooooo! How are you?\")\n",
        "print(my_sp_tok_output.tokens)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁Hell', 'oo', 'oo', 'oo', 'oo', 'o', '!', '▁How', '▁are', '▁you?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEmuqhG0vsV2",
        "colab_type": "text"
      },
      "source": [
        "# Similary, let's train a Byte-Level Tokenizer.\n",
        "\n",
        "`Tokenizer`'s BBPE adds a special token 'Ġ' at the beginning of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Owi-ZJipkJdM",
        "colab_type": "code",
        "outputId": "6d040291-a388-4dfc-9d57-4d056812ac49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# First, initialize a new Byte-level tokenizer\n",
        "my_bbpe_tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# And then train\n",
        "my_bbpe_tokenizer.train([\"botchan.txt\"], vocab_size=20000, min_frequency=2)\n",
        "\n",
        "# Now we can encode\n",
        "my_bbpe_tok_output = my_bbpe_tokenizer.encode(\"Hellooooooooo! How are you?\")\n",
        "print(my_bbpe_tok_output.tokens)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hell', 'oo', 'oo', 'oo', 'oo', 'o', '!', 'ĠHow', 'Ġare', 'Ġyou', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qp9X3XRo9a-C",
        "colab_type": "text"
      },
      "source": [
        "## How can we get back the tokens before BPE?\n",
        "\n",
        "### For word piece:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNnsjSKw8ikx",
        "colab_type": "code",
        "outputId": "55ea4dc2-2861-4cf4-c6c0-8ba933f110cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tmp = ' '.join(my_tok_output.tokens).replace(\" ##\", '')\n",
        "print(\"Tokens before wordpiece: \", tmp)\n",
        "# ['hello', '##oo'] 'hellooo'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokens before wordpiece:  hellooooooooo ! how are you ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJdaAIKV984f",
        "colab_type": "text"
      },
      "source": [
        "For SentencePiece:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrTUBIjE973l",
        "colab_type": "code",
        "outputId": "2df95350-e99c-4465-9d4f-e46b5c2791fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tmp = ''.join(my_sp_tok_output.tokens).replace(\"▁\", ' ')\n",
        "print(\"Tokens before SentencePiece: \", tmp)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokens before SentencePiece:   Hellooooooooo! How are you?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK3n6evYkoNq",
        "colab_type": "text"
      },
      "source": [
        "For byte-level BPE:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOBNixJskb-O",
        "colab_type": "code",
        "outputId": "949cc46e-8c4a-43a6-ebc8-f29a1b3e25bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tmp = ''.join(my_bbpe_tok_output.tokens).replace(\"Ġ\", ' ')\n",
        "print(\"Tokens before BBPE: \", tmp)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokens before BBPE:  Hellooooooooo! How are you?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4yd7AaY-pb0",
        "colab_type": "text"
      },
      "source": [
        "As you can see, it's usually pretty straightforward to de-bpe the bpe tokens. Detokenizing the tokens to revert the pretokenization in WordPiece is a different story."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLTZYFb9xBuu",
        "colab_type": "text"
      },
      "source": [
        "# Implement the original BPE algorithm on your own.\n",
        "\n",
        "### Reference: \n",
        "Sennrich, Rico, Barry Haddow, and Alexandra Birch. \"Neural Machine Translation of Rare Words with Subword Units.\" Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Vol. 1. 2016. http://www.aclweb.org/anthology/P16-1162.\n",
        "\n",
        "Recap: Original BPE expects pretokenized sequence of words as input.  \n",
        "\n",
        "\n",
        "Unlike SentencePiece, it doesn't use a special chracter for space. Instead, it uses an end of word symbol, '</w>' in our case.\n",
        "\n",
        "First, let's create a method that adds '</w>' at the end of each word and convert a word into character tokens with space in between.\n",
        "\n",
        "\"NLP\" -> \"N L P \\</w>\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLqaZfIHN1n7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# toy training data\n",
        "train_sents = [\"what is the higher mountain ?\", \"higher than highest\", \"this is the widest river\", \"newest iphone is cool\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNsPjmkmMo54",
        "colab_type": "code",
        "outputId": "3457c3f2-4abf-4df3-a837-61c8bf3eeaea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import collections, re\n",
        "\n",
        "def get_vocab(sentences):\n",
        "    \"\"\"\n",
        "        This function appends </w> at the end of each word\n",
        "        and add space between characters of the word\n",
        "        and count the occurence of each word\n",
        "    \"\"\"\n",
        "    vocab = collections.defaultdict(int)\n",
        "    for sent in sentences:\n",
        "        words = sent.strip().split()\n",
        "        for word in words:\n",
        "            vocab[' '.join(list(word)) + ' </w>'] += 1\n",
        "    return vocab\n",
        "tmp_vocab = get_vocab(train_sents)\n",
        "print(tmp_vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<class 'int'>, {'w h a t </w>': 1, 'i s </w>': 3, 't h e </w>': 2, 'h i g h e r </w>': 2, 'm o u n t a i n </w>': 1, '? </w>': 1, 't h a n </w>': 1, 'h i g h e s t </w>': 1, 't h i s </w>': 1, 'w i d e s t </w>': 1, 'r i v e r </w>': 1, 'n e w e s t </w>': 1, 'i p h o n e </w>': 1, 'c o o l </w>': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bmWkX1JOdQc",
        "colab_type": "text"
      },
      "source": [
        "We want to merge the most frequently occuring token pairs in our training data.\n",
        "Let's write a function that will return the possible token pairs in our vocabulary and their frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5dMaep9NYBn",
        "colab_type": "code",
        "outputId": "cc28a482-c479-449b-969d-56f15c8edb02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "def get_stats(vocab):\n",
        "    \"\"\"\n",
        "       This function return the consecutive token pairs in our data and their count\n",
        "    \"\"\"\n",
        "    pairs = collections.defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        # add all the consecutive token pairs and their frequencies to counter `pairs`\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols)-1):\n",
        "            pairs[symbols[i],symbols[i+1]] += freq\n",
        "    return pairs\n",
        "\n",
        "tmp_pairs = get_stats(tmp_vocab)\n",
        "tmp_pairs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {('?', '</w>'): 1,\n",
              "             ('a', 'i'): 1,\n",
              "             ('a', 'n'): 1,\n",
              "             ('a', 't'): 1,\n",
              "             ('c', 'o'): 1,\n",
              "             ('d', 'e'): 1,\n",
              "             ('e', '</w>'): 3,\n",
              "             ('e', 'r'): 3,\n",
              "             ('e', 's'): 3,\n",
              "             ('e', 'w'): 1,\n",
              "             ('g', 'h'): 3,\n",
              "             ('h', 'a'): 2,\n",
              "             ('h', 'e'): 5,\n",
              "             ('h', 'i'): 4,\n",
              "             ('h', 'o'): 1,\n",
              "             ('i', 'd'): 1,\n",
              "             ('i', 'g'): 3,\n",
              "             ('i', 'n'): 1,\n",
              "             ('i', 'p'): 1,\n",
              "             ('i', 's'): 4,\n",
              "             ('i', 'v'): 1,\n",
              "             ('l', '</w>'): 1,\n",
              "             ('m', 'o'): 1,\n",
              "             ('n', '</w>'): 2,\n",
              "             ('n', 'e'): 2,\n",
              "             ('n', 't'): 1,\n",
              "             ('o', 'l'): 1,\n",
              "             ('o', 'n'): 1,\n",
              "             ('o', 'o'): 1,\n",
              "             ('o', 'u'): 1,\n",
              "             ('p', 'h'): 1,\n",
              "             ('r', '</w>'): 3,\n",
              "             ('r', 'i'): 1,\n",
              "             ('s', '</w>'): 4,\n",
              "             ('s', 't'): 3,\n",
              "             ('t', '</w>'): 4,\n",
              "             ('t', 'a'): 1,\n",
              "             ('t', 'h'): 4,\n",
              "             ('u', 'n'): 1,\n",
              "             ('v', 'e'): 1,\n",
              "             ('w', 'e'): 1,\n",
              "             ('w', 'h'): 1,\n",
              "             ('w', 'i'): 1})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KvnZWf9RLsr",
        "colab_type": "text"
      },
      "source": [
        "Now, let's write a function that merges the word pairs in vocabulary.\n",
        "Refer to regex library for pattern matching: https://docs.python.org/3/library/re.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mc80uhwWxBAu",
        "colab_type": "code",
        "outputId": "6faac86e-8995-4b0f-f643-2f3cefad8ce9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "def merge_vocab(pair, v_in):\n",
        "    v_out = {}\n",
        "    bigram = re.escape(' '.join(pair))\n",
        "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)') \n",
        "    for word in v_in:\n",
        "        w_out = p.sub(''.join(pair), word)\n",
        "        v_out[w_out] = v_in[word]\n",
        "    return v_out\n",
        "\n",
        "merge_vocab(('h', 'e'), tmp_vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'? </w>': 1,\n",
              " 'c o o l </w>': 1,\n",
              " 'h i g he r </w>': 2,\n",
              " 'h i g he s t </w>': 1,\n",
              " 'i p h o n e </w>': 1,\n",
              " 'i s </w>': 3,\n",
              " 'm o u n t a i n </w>': 1,\n",
              " 'n e w e s t </w>': 1,\n",
              " 'r i v e r </w>': 1,\n",
              " 't h a n </w>': 1,\n",
              " 't h i s </w>': 1,\n",
              " 't he </w>': 2,\n",
              " 'w h a t </w>': 1,\n",
              " 'w i d e s t </w>': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0XPEitLVHSc",
        "colab_type": "text"
      },
      "source": [
        "Finally, the following function is to keep track of tokens in our vocabulary since the beginning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlyPDJMR8-q-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tokens(vocab, tokens=None):\n",
        "    if tokens is None:\n",
        "        tokens = []\n",
        "    for word, freq in vocab.items():\n",
        "        word_tokens = word.split()\n",
        "        tokens.extend(word_tokens)\n",
        "    return list(set(tokens))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKHRT4Ir9e6A",
        "colab_type": "code",
        "outputId": "d070e737-857a-4473-f7a5-70533cda62f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "train_vocab = get_vocab(train_sents)\n",
        "\n",
        "tokens = get_tokens(train_vocab)\n",
        "print(\"vocab before training: \", train_vocab) \n",
        "print(\"number of tokens before training: \", tokens) # All characters"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab before training:  defaultdict(<class 'int'>, {'w h a t </w>': 1, 'i s </w>': 3, 't h e </w>': 2, 'h i g h e r </w>': 2, 'm o u n t a i n </w>': 1, '? </w>': 1, 't h a n </w>': 1, 'h i g h e s t </w>': 1, 't h i s </w>': 1, 'w i d e s t </w>': 1, 'r i v e r </w>': 1, 'n e w e s t </w>': 1, 'i p h o n e </w>': 1, 'c o o l </w>': 1})\n",
            "number of tokens before training:  ['u', 'e', 'h', 'a', 'd', 'p', '?', 'c', 'l', 'm', 'n', 'v', 's', 'g', 'o', 'i', '</w>', 't', 'w', 'r']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9zIS7m84eeY",
        "colab_type": "code",
        "outputId": "2d54e769-82f2-456e-e895-fb96212dd377",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "num_merges = 5\n",
        "for i in range(num_merges):\n",
        "    # Get the token pair with max frequency\n",
        "    pairs = get_stats(train_vocab)\n",
        "    if not pairs:\n",
        "        break\n",
        "    best = max(pairs, key=pairs.get)\n",
        "\n",
        "    # update train_vocab using the merged best pair \n",
        "    train_vocab = merge_vocab(best, train_vocab)\n",
        "\n",
        "    # update tokens using train_vocab\n",
        "    tokens = get_tokens(train_vocab, tokens)\n",
        "\n",
        "    print('Iter: {}'.format(i))\n",
        "    print('Best pair: {}'.format(best))\n",
        "    print('Tokens: {}'.format(tokens))\n",
        "    print('Number of tokens: {}'.format(len(tokens)))\n",
        "    print('==========')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter: 0\n",
            "Best pair: ('h', 'e')\n",
            "Tokens: ['h', 'he', 'n', 'v', 's', 'i', 'r', 'e', '?', 'g', 'o', 'a', 'l', 'd', 'm', 't', 'u', 'p', 'c', '</w>', 'w']\n",
            "Number of tokens: 21\n",
            "==========\n",
            "Iter: 1\n",
            "Best pair: ('t', '</w>')\n",
            "Tokens: ['h', 'he', 'n', 'v', 's', 'i', 'r', 'e', '?', 'g', 't</w>', 'o', 'a', 'l', 'd', 'm', 't', 'u', 'p', 'c', '</w>', 'w']\n",
            "Number of tokens: 22\n",
            "==========\n",
            "Iter: 2\n",
            "Best pair: ('i', 's')\n",
            "Tokens: ['is', 'h', 'he', 'n', 'v', 's', 'i', 'r', 'e', '?', 'g', 't</w>', 'o', 'a', 'l', 'd', 'm', 't', 'u', 'p', 'c', '</w>', 'w']\n",
            "Number of tokens: 23\n",
            "==========\n",
            "Iter: 3\n",
            "Best pair: ('is', '</w>')\n",
            "Tokens: ['is', 'h', 'he', 'n', 'v', 's', 'i', 'r', 'e', '?', 'g', 't</w>', 'o', 'a', 'l', 'is</w>', 'd', 'm', 't', 'u', 'p', 'c', '</w>', 'w']\n",
            "Number of tokens: 24\n",
            "==========\n",
            "Iter: 4\n",
            "Best pair: ('h', 'i')\n",
            "Tokens: ['is', 'h', 'he', 'n', 'v', 's', 'i', 'r', 'e', '?', 'g', 't</w>', 'o', 'a', 'l', 'is</w>', 'd', 'm', 't', 'u', 'p', 'c', 'hi', '</w>', 'w']\n",
            "Number of tokens: 25\n",
            "==========\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzxZzuImhSyC",
        "colab_type": "text"
      },
      "source": [
        "## Questions: \n",
        "- Why does BPE help?\n",
        "- Why is byte-level BPE more compact than the char-level BPE?\n",
        "- Can you have unknown tokens if you use WordPiece, SentencePiece, or byte-level BPE?\n",
        "- Imagine you are building a multilingual NLP system (very different languages with different characters), what kind of tokenization method would you use?"
      ]
    }
  ]
}