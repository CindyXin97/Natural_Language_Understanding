{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab_4_student_in_session.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YhNTG5TiQ1zl","colab_type":"text"},"source":["# Cross-lingual embeddings\n","\n","In this lab we will explore multilingual word embeddings and build a very rudimentary translation system."]},{"cell_type":"markdown","metadata":{"id":"JVHVrUvAQvml","colab_type":"text"},"source":["Adapted from: https://github.com/facebookresearch/MUSE/blob/master/demo.ipynb"]},{"cell_type":"markdown","metadata":{"id":"vJTOC_deXfkb","colab_type":"text"},"source":["## Theory\n","Cross-lingual embedding vectors can be trained in supervised or unsupervised way.   \n","\n","**Supervised.**  \n","\n","First, the embeddings are trained for each language separately.   \n","Then optimization problem of aligning the embeddings is solved using a seed lexicon (small number of aligned pairs of words) by minimizing square loss with cross-domain similarity local scaling (CSLS) criterion.  \n","More details in [Joulin et al (2018)](https://arxiv.org/pdf/1804.07745.pdf)\n","\n","**Unsupervised.**\n","\n","First, a rotation matrix $W$ which roughly aligns the\n","two distributions is learnt using adversarial learning.   \n","Second, the mapping $W$ is further refined: frequent words aligned by the previous step are used as anchor points, and an energy function that corresponds to a spring system between anchor points is minimized (Procrustes method).   \n","Finally, they translate by using the mapping $W$ and a distance metric (CSLS) that expands the space where there is high density of points (like the area around the word\n","“cat”), so that “hubs” (like the word “cat”) become less close to other word vectors than they would otherwise.\n","\n","![](https://drive.google.com/uc?export=view&id=1IuI4NGiUMUtS5whr_mldnsL5tFVoKtwH)\n","\n","\n","More details in [Conneau et al (2018)](https://arxiv.org/pdf/1710.04087.pdf)"]},{"cell_type":"markdown","metadata":{"id":"w68QNhoOXowy","colab_type":"text"},"source":["## Practice"]},{"cell_type":"markdown","metadata":{"id":"psrhJi6wOE0W","colab_type":"text"},"source":["### Load embeddings\n","Here we load a subset of 100,000 + 100,000 aligned [fastText embeddings](https://fasttext.cc/docs/en/aligned-vectors.html) for English and Russian languages. "]},{"cell_type":"code","metadata":{"id":"I6oKVHW8ZxGS","colab_type":"code","colab":{}},"source":["!wget 'https://drive.google.com/uc?export=download&id=1-Hrc2uz14kmcsKYle7_penmpR7t0TtZR' -O en_embeddings.npz\n","!wget 'https://drive.google.com/uc?export=download&id=1-ZnGxODZypEnz5E0ssXLCEMG-fSfU28W' -O en_word2id.p\n","\n","!wget 'https://drive.google.com/uc?export=download&id=1-OE9Tw8M5jWvM-4WRKfladaQzLIgoxiT' -O ru_embeddings.npz\n","!wget 'https://drive.google.com/uc?export=download&id=1-Y42yEnIsrQtVdQ7PABrvrn4eYRKmyuG' -O ru_word2id.p\n","\n","!wget 'https://drive.google.com/uc?export=download&id=1-TsynEry2jdbIY2P3_c7UjHdbwr345Nf' -O secret_embeddings.npy"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OIG-PHMSOHif","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pickle\n","import pandas as pd"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PAD_ImoyE-Ce","colab_type":"code","colab":{}},"source":["# load the files\n","x = np.load('en_embeddings.npz', allow_pickle=True)\n","en_embeddings = [x[k] for k in x][0]\n","\n","x = np.load('ru_embeddings.npz', allow_pickle=True)\n","ru_embeddings = [x[k] for k in x][0]\n","\n","with open('en_word2id.p', 'rb') as handle:\n","    en_word2id = pickle.load(handle)\n","\n","with open('ru_word2id.p', 'rb') as handle:\n","    ru_word2id = pickle.load(handle)\n","\n","# create id2word for both languages\n","en_id2word = [None] * len(en_word2id)\n","for word, idx in en_word2id.items():\n","    en_id2word[idx] = word\n","ru_id2word = [None] * len(ru_word2id)\n","for word, idx in ru_word2id.items():\n","    ru_id2word[idx] = word"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5RAP8Wr3nn_t","colab_type":"text"},"source":["### Visualize multilingual embeddings"]},{"cell_type":"markdown","metadata":{"id":"wE7KAMi0gme0","colab_type":"text"},"source":["Let's visualize the embeddings. We take pairs of words that have same meaning, where one is English and the other is Russian. As they exist in 300-dimensional space which is hard to imagine, we need to project them to a 2D space. We will use the first two components of PCA to do this. "]},{"cell_type":"code","metadata":{"id":"TwNSpR0vnok2","colab_type":"code","colab":{}},"source":["from sklearn.decomposition import PCA\n","pca = PCA(n_components=2, whiten=True)\n","pca.fit(np.vstack([en_embeddings, ru_embeddings]))\n","print('Variance explained: %.2f' % pca.explained_variance_ratio_.sum())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NWOAR-7an63m","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","\n","def plot_similar_word(src_words, src_word2id, src_emb, tgt_words, tgt_word2id, tgt_emb, pca):\n","\n","    Y = []\n","    word_labels = []\n","    for sw in src_words:\n","        Y.append(src_emb[src_word2id[sw]])\n","        word_labels.append(sw)\n","    for tw in tgt_words:\n","        Y.append(tgt_emb[tgt_word2id[tw]])\n","        word_labels.append(tw)\n","\n","    # find PCA coords for 2 dimensions\n","    Y = pca.transform(Y)\n","    x_coords = Y[:, 0]\n","    y_coords = Y[:, 1]\n","\n","    # display scatter plot\n","    plt.figure(figsize=(10, 8), dpi=80)\n","    plt.scatter(x_coords, y_coords, marker='x')\n","\n","    for k, (label, x, y) in enumerate(zip(word_labels, x_coords, y_coords)):\n","        color = 'blue' if k < len(src_words) else 'red'  # src words in blue / tgt words in red\n","        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points', fontsize=19,\n","                     color=color, weight='bold')\n","        \n","    for k in range(len(src_words)):\n","        idx_src = k\n","        idx_tgt = k + len(src_words)\n","        plt.plot([x_coords[idx_src], x_coords[idx_tgt]], [y_coords[idx_src], y_coords[idx_tgt]])\n","\n","    plt.xlim(x_coords.min() - 0.2, x_coords.max() + 0.2)\n","    plt.ylim(y_coords.min() - 0.2, y_coords.max() + 0.2)\n","    plt.title('Visualization of the multilingual word embedding space')\n","\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LA69RSian-uQ","colab_type":"code","colab":{}},"source":["# get 5 random input words\n","en_words = ['university', 'love', 'history', 'tennis', 'research', 'conference']\n","ru_words = ['университет', 'любовь', 'история', 'теннис', 'исследование', 'конференция']\n","\n","# assert words in dictionaries\n","for en_word in en_words:\n","    assert en_word in en_word2id, '\"%s\" not in source dictionary' % en_word\n","for ru_word in ru_words:\n","    assert ru_word in ru_word2id, '\"%s\" not in target dictionary' % ru_word\n","\n","plot_similar_word(en_words, en_word2id, en_embeddings, ru_words, ru_word2id, ru_embeddings, pca)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sr9E8edNX_C4","colab_type":"text"},"source":["### Get nearest neigbors for a given word"]},{"cell_type":"markdown","metadata":{"id":"jCoy6RqxhmLF","colab_type":"text"},"source":["Let's write a function that given a word, returns its K nearest neighbors in the vector space."]},{"cell_type":"code","metadata":{"id":"vvzNQe45QTU4","colab_type":"code","colab":{}},"source":["def get_nn(word, src_emb, src_word2id, tgt_emb, tgt_id2word, K=5):\n","    # 1. Look up the word embedding\n","    word_emb = src_emb[src_word2id[word]]\n","\n","    # 2. Compute the scores for each word\n","    scores = tgt_emb.dot(word_emb)\n","\n","    # 3. Find the index of the top K best scoring words\n","    # 4. Get the corresponding top K words\n","    k_best = pd.Series(scores, index=tgt_id2word).sort_values(ascending=False)[:K]\n","\n","    return k_best\n","\n","def print_k_best_for_word(k_best, word):\n","    print(\"Nearest neighbors of \\\"%s\\\":\" % word)\n","    for word, score in k_best.items():\n","        print('%.4f - %s' % (score, word))\n","\n","    print()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JThMzwplyRPB","colab_type":"code","colab":{}},"source":["en_words = [\"algorithm\", \"language\", \"research\"]\n","for word in en_words:\n","    k_best = get_nn(word, en_embeddings, en_word2id, en_embeddings, en_id2word, K=6)\n","    print_k_best_for_word(k_best, word)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"l783sUP-gxIJ","colab":{}},"source":["en_words = [\"hello\", \"world\"]\n","for word in en_words:\n","    k_best = get_nn(word, en_embeddings, en_word2id, en_embeddings, en_id2word, K=3)\n","    print_k_best_for_word(k_best, word)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t8F58UYwglci","colab_type":"text"},"source":["We can also search in Russian and find the closest words in English!"]},{"cell_type":"code","metadata":{"id":"Y9G_AZmOVmma","colab_type":"code","colab":{}},"source":["ru_words = [\"привет\", \"мир\"]\n","for word in ru_words:\n","    k_best = get_nn(word, ru_embeddings, ru_word2id, en_embeddings, en_id2word, K=3)\n","    print_k_best_for_word(k_best, word)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6aJv0gIpbkRr","colab_type":"code","colab":{}},"source":["en_words = [\"hello\", \"world\"]\n","for word in en_words:\n","    k_best = get_nn(word, en_embeddings, en_word2id, ru_embeddings, ru_id2word, K=3)\n","    print_k_best_for_word(k_best, word)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PAG-_KufYOV4","colab_type":"text"},"source":["### A simple word-to-word translation system"]},{"cell_type":"markdown","metadata":{"id":"rDeYkM_dbKFr","colab_type":"text"},"source":["We can try to use aligned embeddings to build a very rudimentary translation system.\n","\n","Here is a list of texts in Russian. We will parse it, convert to lowercase, remove all special symbols, and split to words.   \n","Then for each word, if it exists in `ru_word2id`, translate it to english (using closest english word), otherwise skip it."]},{"cell_type":"code","metadata":{"id":"L0azS1XLqw5X","colab_type":"code","colab":{}},"source":["ru_texts = [\n","\"\"\"Игровое действие в американском футболе состоит из серии коротких по продолжительности отдельных схваток, за пределами которых мяч называют «мертвым» или не в игре. Во время схватки могут быть разыграны:\n","пасовая комбинация,\n","выносная комбинация,\n","пант ( удар по мячу ),\n","попытка взятия зачетной зоны\n","свободный удар (ввод мяча в игру – начальный удар)\n","Цель игры – набрать максимальное количество очков, занеся мяч в зачетную зону противника (тачдаун - touchdown) или забив его в ворота с поля (филд-гол – field goals). Побеждает команда, набравшая наибольшее количество очков.\"\"\",\n","\"\"\"Я вас любил: любовь ещё, быть может,\n","В душе моей угасла не совсем;\n","Но пусть она вас больше не тревожит;\n","Я не хочу печалить вас ничем.\n","Я вас любил безмолвно, безнадежно,\n","То робостью, то ревностью томим;\n","Я вас любил так искренно, так нежно,\n","Как дай вам Бог любимой быть другим\"\"\",\n","\"\"\"Сегодня мы говорим про слова и стоит обсудить, как делать такое сопоставление вектора слову.\n","Вернемся к предмету: вот у нас есть слова и есть компьютер, который должен с этими словами как-то работать. Вопрос — как компьютер будет работать со словами? Ведь компьютер не умеет читать, и вообще устроен сильно иначе, чем человек. Самая первая идея, приходящая в голову — просто закодировать слова цифрами по порядку следования в словаре.\"\"\",\n","\"\"\"Интернет-мем — информация в той или иной форме (медиаобъект, то есть объект, создаваемый электронными средствами коммуникации, фраза, концепция или занятие), как правило, остроумная и ироническая[2], спонтанно приобретающая популярность, распространяясь в Интернете разнообразными способами (посредством социальных сетей, форумов, блогов, мессенджеров и пр.). Обозначает также явление спонтанного распространения такой информации или фразы. \n","Мемами могут считаться как слова, так и изображения. Иначе говоря, это любые высказывания, картинки, видео или звукоряд, которые имеют значение и устойчиво распространяются во Всемирной паутине.\"\"\",\n","]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zQQNJK4WRMwi","colab_type":"code","colab":{}},"source":["import re\n","\n","for ru_text in ru_texts:\n","    ru_text = re.sub(',(?!\\s+\\d$)', '', ru_text).lower()\n","    translation = []\n","\n","    for word in re.split(r'(\\s+)', ru_text):\n","        # Your code goes here\n","        if word.isspace():\n","            candidate = word\n","        elif word in ru_word2id:\n","            k_best = get_nn(word, ru_embeddings, ru_word2id, en_embeddings, en_id2word, K=1)\n","            candidate = list(k_best.keys())[0]\n","        else:\n","            candidate = word\n","        \n","        translation.append(candidate)\n","\n","    print(\"\".join(translation))\n","    print(\"\\n===\\n\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NEC7mdwpeFt9","colab_type":"text"},"source":["### Bonus: Find the missing words!\n","\n","We secretly removed four English words from the English embedding data! However, we saved their embedding vectors. Can you recover the missing words?\n","\n","*Hint: Find the nearest neighbors for each vector.*\n","\n","*Hint: You can test if your guesses are correct be seeing if the word exists in the `en_word2id` dictionary.*\n","\n","Once you have found them, you can DM me (Jason) on Campuswire with the missing words. Remember to include your NetID."]},{"cell_type":"code","metadata":{"id":"B6VZh05icVAm","colab_type":"code","colab":{}},"source":["file_name = \"secret_embeddings.npy\"\n","secret_embeddings = np.load(file_name, allow_pickle=True)\n","secret_embeddings.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1o_CdZrwUATp","colab_type":"code","colab":{}},"source":["# Your code goes here\n","raise NotImplementedError"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c2OUw6AzUFda","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}